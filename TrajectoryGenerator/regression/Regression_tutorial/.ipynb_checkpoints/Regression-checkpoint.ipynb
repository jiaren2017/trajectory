{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "\n",
    "There are two popular types to fit or train a supervied learning model as follows:\n",
    "- \"Classification\" is the task of predicting a discrete class label.\n",
    "- \"Regression\" is the task of predicting a continuous quantity.\n",
    "\n",
    "\n",
    "This tutorial will focus on the regression, which covers an indroduction to **linear regression**, **locally weighted linear regression** and the **tree-based regression**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression\n",
    "\n",
    "The goal of using regression is to predict a numeric target value. One way to do this is to write out an equation for the target value with respect to the inputs as follows: \n",
    "$$HousePrice = \\omega_1 * House Size  +  \\omega_2 * Age of House \\tag{1}$$\n",
    "\n",
    "In the regression equation above, the \"$\\omega_1$\"and \"$\\omega_2$\" are known as regression **weights** and \"house size\" and \"age of house\" are the **inputs**. Once we have those data, we will be able to predict new value by multiplying the inputs by the regression weights and add them together to get a forecast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we deal with a bunch of data? Let's assume that our input and output data are matrix X and Y:\n",
    "\n",
    "$$ X = \\begin{bmatrix}\n",
    "    x_{11}   & x_{12}  & x_{13}\\\\\n",
    "    x_{21}  & x_{22} & x_{23} \\\\\n",
    "    x_{31}  & x_{32} & x_{33} \n",
    "\\end{bmatrix} \\;\\;\\;      \n",
    "Y = \\begin{bmatrix}\n",
    "    y_{1}\\\\\n",
    "    y_{2}\\\\\n",
    "    y_{3} \n",
    "\\end{bmatrix} \\;\\;\\; $$\n",
    "\n",
    "and we put regression weights into a vector:\n",
    "$$ \\omega = \\begin{bmatrix}\n",
    "    \\omega_{1}\\\\\n",
    "    \\omega_{2}\\\\\n",
    "    \\omega_{3} \n",
    "\\end{bmatrix} \\;\\;\\; $$\n",
    "\n",
    "then we could predict the value $y_1$ for the given dataset $x_1$, namely the first column of matrix X:\n",
    "\n",
    "$$ y_1 = \\; \\begin{bmatrix}\n",
    "    x_{11}\\\\\n",
    "    x_{21}\\\\\n",
    "    x_{31} \n",
    "\\end{bmatrix}^T \\; * \\;\\; \n",
    "\\begin{bmatrix}\n",
    "    \\omega_{1}\\\\\n",
    "    \\omega_{2}\\\\\n",
    "    \\omega_{3} \n",
    "\\end{bmatrix} \\;\\;\\; \\tag{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next question is how to find the weights with the given training datasets. One way is to find such weights, with which the difference between actual value and predicted value could be minimized. Usually we use the squared error (convex function) that can provide a gloable minimum.\n",
    "$$ \\sum_{i=1}^m (y_i - x_i^T\\omega)^2 $$\n",
    "\n",
    "The corresponding expression in matrix is \n",
    "$$ (Y-X\\omega)^T(Y-X\\omega) $$\n",
    "\n",
    "After the derivation, we get \n",
    "$$ X^T(Y-X\\omega)$$\n",
    "\n",
    "Setting this to zero and solve for $\\omega$ to get the following equation:\n",
    "\n",
    "$$ \\hat{\\omega} = (X^TX)^{-1}X^TY \\tag{3}$$\n",
    "\n",
    "The little hat on the top of $\\omega$ means that this is the best weight for the training data. It may not perfectly work for other data, so we use a \"*hat*\" to describe our best estimate given the data. Another thing to note is that the equation above uses term $(X^TX)^{-1}$ which is a matrix inverse. We should make sure that the matrix inverse exists and then put it into code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to learn how to create a simple best-fit line for a certain dataset.\n",
    "Let's first import all the packages that we need for the tasks and add the data-loading function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataSet(fileName):\n",
    "    numFeat = len(open(fileName).readline().split('\\t')) - 1\n",
    "    xArr = []; yArr = []\n",
    "    fr = open(fileName)\n",
    "    for line in fr.readlines():\n",
    "        lineArr =[]\n",
    "        curLine = line.strip().split('\\t')\n",
    "        for i in range(numFeat):\n",
    "            lineArr.append(float(curLine[i]))\n",
    "        xArr.append(lineArr)\n",
    "        yArr.append(float(curLine[-1]))\n",
    "    return xArr, yArr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the training data and see how they look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x, train_set_y = loadDataSet(\"./input/trainingData.txt\")\n",
    "print(\"train_set_x[0:5]:\\n\",np.reshape(train_set_x[0:5],(-1,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the first column of input matrix is always 1.0 which represents a constant offset, while the second column shows different values. Since we have all the training data, let's introduce another function which will implement the equation(3) to calculate the weights after checking the inversability of $(X^TX)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standRegres(xArr,yArr):\n",
    "    xMat = np.mat(xArr); yMat = np.mat(yArr).T\n",
    "    xTx = xMat.T * xMat                            \n",
    "    if np.linalg.det(xTx) == 0.0:\n",
    "        print(\"This matrix is singular, cannot do inverse\")\n",
    "        return\n",
    "    ws = xTx.I * (xMat.T*yMat)\n",
    "    return ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = standRegres(train_set_x, train_set_y)\n",
    "print(\"weights:\\n\", ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's predict the value $\\hat{Y}$ with our weights and input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_xMat=np.mat(train_set_x)\n",
    "train_yMat=np.mat(train_set_y)\n",
    "xCopy = train_xMat.copy()                 \n",
    "xCopy.sort(0)                # sort the points in ascending order for pyplot\n",
    "y_hat = xCopy*ws             # prediction with ordered x_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the training set and our best-fit line to see how they look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(train_xMat[:,1].flatten().A[0], train_yMat.T[:,0].flatten().A[0], s=20, c='blue', alpha=.5)\n",
    "ax.plot(xCopy[:,1], y_hat, c='red')\n",
    "plt.title('Linear Regression')\n",
    "plt.xlabel('X')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph above we can see that the training set and best-fit line are showing the similar trace to some extent. But how far do they differ from each other? We are going to apply the function *corrcoef()* to calculate the correlation of those two vaules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = train_xMat*ws              # prediction with originally ordered x_matrix\n",
    "np.corrcoef(y_hat.T, train_yMat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The elements on the diagonal are 1.0 because the correlation between *train_yMat* and itself is perfect. The correlation between our prediction(*y_hat*) and actual value(*train_yMat*) is 0.98, which shows a relatively good result of our prediction. On the other hand, the linear regression fails to fit the data quite well, which is known as **underfitting** problem.\n",
    "Next we will introduce one way to deal with this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locally weighted linear regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to solve the underfitting problem is to reduce the mean-squared error by adding some bias into estimator, which is a technique known as **locally weighted linear regression** (*LWLR*). In LWLR more weights are given to those data points which are close to the data point of interest; then the least-squares regression similar to the linear regression will be carried out. The following equation shows the calculation of weights:\n",
    "\n",
    "$$ \\hat{\\omega} = (X^TWX)^{-1}X^TWY \\tag{4}$$\n",
    "\n",
    "where the W is a matrix and will be generated by a *kernel* function, which shall give nearby points more weights than other points. The mostly used kernel is *Gaussian* and assigns the weights by\n",
    "$$ \\omega(i,i) = exp\\bigg(\\frac{\\big|x^i-x\\big|}{-2\\kappa^2}\\bigg) \\tag{5}$$\n",
    "\n",
    "According to the equation, the closer the data point x is to the other points, the larger $\\omega$(i,i) will be. Besides, there is also one constant value $\\kappa$ that determines how much to weight the nearby points. It will be shown in the graph below how the weights matrix is changed by different $\\kappa$.\n",
    "\n",
    "\n",
    "![kernel](img/kernel.jpg)\n",
    "\n",
    "The figure above shows the original data in the top frame and the weights applied to each piece of data (here we are forecasting the value of x=0.5). With $\\kappa$=0.5, most of the data are included, while only a few local points will be used when the $\\kappa$=0.01\n",
    "\n",
    "Now let's edit the \"lwlr\" function for locally weighted linear regression and one test function to evaluate the size of $\\kappa$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lwlr(testPoint, xArr, yArr, k = 1.0):\n",
    "    xMat = np.mat(xArr); yMat = np.mat(yArr).T\n",
    "    m = np.shape(xMat)[0]\n",
    "    weights = np.mat(np.eye((m)))              # Create diagonal matrix\n",
    "    for j in range(m):                         # Populate weights with exponentially decaying values\n",
    "        diffMat = testPoint - xMat[j, :]                                 \n",
    "        weights[j, j] = np.exp(diffMat * diffMat.T/(-2.0 * k**2))\n",
    "    xTx = xMat.T * (weights * xMat)                                        \n",
    "    if np.linalg.det(xTx) == 0.0:\n",
    "        print(\"This matrix is singular, cannot do inverse\")\n",
    "        return\n",
    "    ws = xTx.I * (xMat.T * (weights * yMat))   # calculate the weights for regression\n",
    "    return testPoint * ws                      # return the predicted value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lwlrTest(testArr, xArr, yArr, k=1.0):      # call lwlr() to predict the y_hat\n",
    "    m = np.shape(testArr)[0]                   # gives the size of data set\n",
    "    y_hat = np.zeros(m)    \n",
    "    for i in range(m):                         # predict for each sample\n",
    "        y_hat[i] = lwlr(testArr[i],xArr,yArr,k)\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the locally weighted linear regression with different kernel function($\\kappa$ = 1.0, 0.01, 0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_1 = lwlrTest(train_set_x, train_set_x, train_set_y, k=1.0)                           \n",
    "y_hat_2 = lwlrTest(train_set_x, train_set_x, train_set_y, k=0.01)                          \n",
    "y_hat_3 = lwlrTest(train_set_x, train_set_x, train_set_y, k=0.003)                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate the result, let's plot these estimates with the original values. Plot needs the data to be sorted, so let's sort train_xMat first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srtInd = train_xMat[:,1].argsort(0)  # return the ordered index according to the value of train_xMat[:,1]\n",
    "xSort=train_xMat[srtInd][:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=3, ncols=1,sharex=False, sharey=False, figsize=(10,8))\n",
    "axs[0].plot(xSort[:, 1], y_hat_1[srtInd], c = 'red')                        # plot the regression line\n",
    "axs[1].plot(xSort[:, 1], y_hat_2[srtInd], c = 'red')                        # plot the regression line\n",
    "axs[2].plot(xSort[:, 1], y_hat_3[srtInd], c = 'red')                        # plot the regression line\n",
    "axs[0].scatter(train_xMat[:,1].flatten().A[0], train_yMat.flatten().A[0], s = 20, c = 'blue', alpha = .5)  # plot samples\n",
    "axs[1].scatter(train_xMat[:,1].flatten().A[0], train_yMat.flatten().A[0], s = 20, c = 'blue', alpha = .5)                \n",
    "axs[2].scatter(train_xMat[:,1].flatten().A[0], train_yMat.flatten().A[0], s = 20, c = 'blue', alpha = .5) \n",
    "axs0_title_text = axs[0].set_title('Locally weighted linear regression, k=1.0')     # set title\n",
    "axs1_title_text = axs[1].set_title('Locally weighted linear regression, k=0.01')\n",
    "axs2_title_text = axs[2].set_title('Locally weighted linear regression, k=0.003')\n",
    "plt.setp(axs0_title_text, size=8, weight='bold', color='red')  \n",
    "plt.setp(axs1_title_text, size=8, weight='bold', color='red')  \n",
    "plt.setp(axs2_title_text, size=8, weight='bold', color='red')  \n",
    "plt.xlabel('X')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure above shows that as the $\\kappa$ is getting smaller, a better fitting line will be generated. But when the $\\kappa$ becomes too small, we will have **overfitting** problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the age of an abalone\n",
    "\n",
    "Since we have learned regression algorithm, let's practice it on some real live data. In the file *\"abalone.txt\"*, there are some data from the UCI data repository describing the age of a shellfish called abalone. The year is known by counting the number of layers in the shell of the abalone. Let's first load the dataset and see how they look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abX, abY = loadDataSet('./input/abalone.txt')\n",
    "print(\"abX[0:5]:\\n\",np.reshape(abX[0:5],(5,-1)))\n",
    "print(\"abY[0:5]:\\n\",np.reshape(abY[0:5],(5,-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the *abX*, there are values for eight features while the we can find the age in *abY*.\n",
    "Now let's first use the locally weighted linear regression method to find our weights based on the training set *abX[0:99]* and *abY[0:99]* and then evaluate the estimates with the same dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yHat01 = lwlrTest(abX[0:99], abX[0:99], abY[0:99], k=0.1)\n",
    "yHat1  = lwlrTest(abX[0:99], abX[0:99], abY[0:99], k=1)\n",
    "yHat10 = lwlrTest(abX[0:99], abX[0:99], abY[0:99], k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the evaluation we will use the following function *rssErro()*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rssError(yArr, yHatArr):\n",
    "    return ((yArr - yHatArr) **2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training set and test set are identical, the effects of kernel on the estimates are:')\n",
    "print('for k=0.1, the Error:',rssError(abY[0:99], yHat01.T))\n",
    "print('for k=1,   the Error:',rssError(abY[0:99], yHat1.T))\n",
    "print('for k=10,  the Error:',rssError(abY[0:99], yHat10.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results above we find that a smaller kernel leads to a lower error on the same dataset. But we have learned that too small kernel will overfit our data, which may or may not give us the best estimation on new datasets. Let's see how they work on the new datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yHat01 = lwlrTest(abX[100:199], abX[0:99], abY[0:99], k=0.1)\n",
    "yHat1  = lwlrTest(abX[100:199], abX[0:99], abY[0:99], k=1)\n",
    "yHat10 = lwlrTest(abX[100:199], abX[0:99], abY[0:99], k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training set and test set are different, the effects of kernel on the estimates are:')\n",
    "print('for k=0.1, the Error:',rssError(abY[100:199], yHat01.T))\n",
    "print('for k=1,   the Error:',rssError(abY[100:199], yHat1.T))\n",
    "print('for k=10,  the Error:',rssError(abY[100:199], yHat10.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the results, the overfitting with smallest kernel caused the most error on new testsets. The kernel of 10 gives the smallest error on new data while it results in the biggest error on training set. Let's see how these erros compare to our simple linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training set and test set are different, linear regression vs. locally weighted linear regression with k=1:')\n",
    "print('lwlr with k=1, the Error:', rssError(abY[100:199], yHat1.T))\n",
    "ws = standRegres(abX[0:99], abY[0:99])\n",
    "yHat = np.mat(abX[100:199]) * ws\n",
    "print('linear regression, the Error:', rssError(abY[100:199], yHat.T.A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple linear regression works almost as well as the locally weighted linear regression. This demonstration illustrates one fact: in order to find the best model, we have to see how the model works on **unknown** data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Till now we have learned *linear regression* and *locally weighted linear regression*. Our example shows that locally weighted linear regression sometimes work better than regular regression. But the problem is that we need to keep the training set available each time when we make the forecast based on new data. Next we will learn another algorithm which can handle the *nonlinearities* in real life."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree-based regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we are going to show how to build a model with *tree regression*, which implements a new algorithm called **CART**(Classification And Regression Trees). It is well-known and well-documented tree-building algorithm that makes binary splits to handle continuous variables. By doing this we choose a feature and make values greater than the desired go on the left side of the tree and all the other values go on the right side. Now we are going to illustrate it with an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binSplitDataSet(dataSet, feature, value):\n",
    "    mat0 = dataSet[np.nonzero(dataSet[:,feature] <= value)[0],:]\n",
    "    mat1 = dataSet[np.nonzero(dataSet[:,feature] > value)[0],:]\n",
    "    return mat0, mat1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testMat = np.mat(np.eye(4))\n",
    "testMat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just created a test matrix and assume that each column represents a certain feature. Now let's split it by the value of a given feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat0, mat1 = binSplitDataSet(testMat, 1, 0.5) \n",
    "print('mat0:\\n', mat0)\n",
    "print('mat1:\\n', mat1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split the test matrix into mat0 and mat1. The feature value of interest in mat0 is greater than 0.5 while those in mat1 are smaller. Since we know how to make a binary split, we'd like to find out which feature and which value to choose for the split. Let's assume that we choose feature *$x_j$* as the splitting variable and the feature value *s* as splitting point, so that we divide data into two regions $R_1$ and $R_2$ \n",
    "\n",
    "$$ R_1(j,s) = \\{x\\big|x^{(j)}\\leq s\\}, \\quad R_2(j,s) = \\{x\\big|x^{(j)}> s\\} \\tag{6}$$\n",
    "\n",
    "Then we calculate the average value for each generated region by\n",
    "$$ \\hat{c}_1 = ave(y_i\\big|x_i\\in R_1(j,s)),\\quad \\hat{c}_2 = ave(y_i\\big|x_i\\in R_2(j,s))\\tag{7}$$\n",
    "\n",
    "In order to get the optimal splitting variable *j* and the optimal splitting point *s*, we must find such ($\\hat{c}_1$, $\\hat{c}_2$) which gives the minimum of total squared error as follows\n",
    "$$ \\mathop \\min_{j,s}\\;[ \\mathop \\min_{\\hat{c}_1} \\sum_{x_i\\in R_1(j,s)}(y_i-\\hat{c}_1)^2 + \\mathop \\min_{\\hat{c}_2} \\sum_{x_i\\in R_2(j,s)}(y_i-\\hat{c}_2)^2] \\tag{8}$$\n",
    "\n",
    "Since we have to go through all the features of data set, the pseudo-code of choosing best split would look like this:  \n",
    ">_For every feature:_  \n",
    "$\\quad$_For every unique value:_  \n",
    "    $\\quad\\quad$_Split the dataset into two_  \n",
    "    $\\quad\\quad$_Measure the error of these two splits_  \n",
    "    $\\quad\\quad$_If the error is less than bestError, then bestSplit to this split and update bestError_  \n",
    ">_Return bestSplit feature and threshold_  \n",
    "\n",
    "Now let's create the code for these functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regLeaf(dataSet):  # calculate the mean value as the model for a leaf node\n",
    "    return np.mean(dataSet[:,-1])  \n",
    "\n",
    "def regErr(dataSet):   # calculate the total squared error of the target variables in a given dataset\n",
    "    return np.var(dataSet[:,-1]) * np.shape(dataSet)[0]  # var = mean(abs(x - x.mean())**2)\n",
    "\n",
    "def chooseBestSplit(dataSet, leafType = regLeaf, errType = regErr, ops = (1,4)):\n",
    "    tolS = ops[0]; tolN = ops[1]  # tolS: tolerance on the error reduction, tolN: minimum data instances to include in a split\n",
    "    if len(set(dataSet[:,-1].T.tolist()[0])) == 1:   # exit if all values are equal\n",
    "        return None, leafType(dataSet)\n",
    "    m, n = np.shape(dataSet)     # get the size of dataset\n",
    "    S = errType(dataSet)         # setting the last feature as the best split and estimate its error for further compare\n",
    "    # initialize bestError, bestIndex for splitting variable and best splitting point\n",
    "    bestS = float('inf'); bestIndex = 0; bestValue = 0\n",
    "  \n",
    "    for featIndex in range(n - 1):   # iterate all feature columns\n",
    "        for splitVal in set(dataSet[:,featIndex].T.tolist()[0]):  # iterate all values of a certain feature\n",
    "            mat0, mat1 = binSplitDataSet(dataSet, featIndex, splitVal) \n",
    "            if (np.shape(mat0)[0] < tolN) or (np.shape(mat1)[0] < tolN): continue  \n",
    "            newS = errType(mat0) + errType(mat1)      # calculate actual error\n",
    "            if newS < bestS:                          # update data if actual error is smaller than best error\n",
    "                bestIndex = featIndex\n",
    "                bestValue = splitVal\n",
    "                bestS = newS\n",
    "    if (S - bestS) < tolS:                            # exit if low error reduction\n",
    "        return None, leafType(dataSet)\n",
    "    mat0, mat1 = binSplitDataSet(dataSet, bestIndex, bestValue)   # make the best split\n",
    "    if (np.shape(mat0)[0] < tolN) or (np.shape(mat1)[0] < tolN):  # exit if split creates small dataset\n",
    "        return None, leafType(dataSet)                # return no feature-ID, only the mean value of data set.\n",
    "\n",
    "    return bestIndex, bestValue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we finish editing the code for regression tree. Let's look at the data that we are going to analyse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataSet(fileName):\n",
    "    dataMat = []\n",
    "    fr = open(fileName)\n",
    "    for line in fr.readlines():\n",
    "        curLine = line.strip().split('\\t')\n",
    "        fltLine = list(map(float, curLine))  # map data to float()\n",
    "        dataMat.append(fltLine)\n",
    "    return dataMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSet = loadDataSet(\"./input/data1.txt\")\n",
    "n = len(dataSet)                                                    \n",
    "xcord = []; ycord = []                                                \n",
    "for i in range(n):                                                    \n",
    "    xcord.append(dataSet[i][0]); ycord.append(dataSet[i][1])        \n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)                                            \n",
    "ax.scatter(xcord, ycord, s = 20, c = 'blue',alpha = .5)                \n",
    "plt.title('DataSet')                                                \n",
    "plt.xlabel('X')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the figure above we can see that this is a simple dataset with two groups. Let's generate a regression tree from this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mat = np.mat(dataSet)\n",
    "split_feat, split_value = chooseBestSplit(data_mat, regLeaf, regErr, (1, 4))\n",
    "print(\"splitting feature:\", split_feat)\n",
    "print(\"splitting point:\", split_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result we know that the best splitting feature is the first column of features while the best splitting value is 0.48813. With this value we can keep the error minimum. Since we already find the splitting feature and value, let's build the regression tree. The principle is straightforward. We will split the whole dataset into two with these optimum splitting values and then build left and right subtree. The pseudo-code for *createTree()* would look like this:\n",
    ">_Find the best feature to split on:_  \n",
    "$\\quad$_If we can’t split the data, this node becomes a leaf node_  \n",
    "$\\quad$_Make a binary split of the data_  \n",
    "$\\quad$_Call createTree() on the right split of the data_  \n",
    "$\\quad$_Call createTree() on the left split of the data_  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTree(dataSet, leafType = regLeaf, errType = regErr, ops = (1, 4)):\n",
    "    feat, val = chooseBestSplit(dataSet, leafType, errType, ops)\n",
    "    if feat == None: return val    # return leaf value if terminating condition met\n",
    "    retTree = {}\n",
    "    retTree['spInd'] = feat\n",
    "    retTree['spVal'] = val\n",
    "    left_Set, right_Set = binSplitDataSet(dataSet, feat, val)\n",
    "    retTree['left'] = createTree(left_Set, leafType, errType, ops)\n",
    "    retTree['right'] = createTree(right_Set, leafType, errType, ops)\n",
    "    return retTree  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(createTree(data_mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the result, the tree has only two leaf nodes, which also matches the dataset.  \n",
    "Let's try this out on some data with more splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSet = loadDataSet(\"./input/data2.txt\")\n",
    "n = len(dataSet)                                                    \n",
    "xcord = []; ycord = []                                                \n",
    "for i in range(n):                                                    \n",
    "    xcord.append(dataSet[i][1]); ycord.append(dataSet[i][2])        \n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)                                            \n",
    "ax.scatter(xcord, ycord, s = 20, c = 'blue',alpha = .5)                \n",
    "plt.title('DataSet')                                                \n",
    "plt.xlabel('X')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Piecewise constant data are shown in the figure above. Let's try to built a regression tree on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mat = np.mat(dataSet)\n",
    "print(createTree(data_mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This forecast does a good job, because the tree shows five leaf nodes, which exactly match the dataset. Now that we are able to build regression trees, we need to find a way to check if we have been doing something wrong. We will next examine *tree pruning*, which modifies our decision trees so that we can make better predictions.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree pruning\n",
    "\n",
    "In the section of liniear regression we have encountered some overfitting problems. Trees with too many nodes could also lead to the model overfit. The procedure of reducing the complexity of a decision tree to avoid overfitting is known as *pruning*. In the function *chooseBestSplit()* we have already done so called *prepruning* by using the stopping conditions. Another form of pruning involves a test set and a training set. This is known as *postpruning* and we will investigate its effectiveness later. Now let's first discuss some of the drawbacks of *prepruning*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepruning\n",
    "\n",
    "In order to illustrate the problem, let's take a look at another data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSet = loadDataSet(\"./input/data3.txt\")\n",
    "n = len(dataSet)                                                    \n",
    "xcord = []; ycord = []                                                \n",
    "for i in range(n):                                                    \n",
    "    xcord.append(dataSet[i][0]); ycord.append(dataSet[i][1])        \n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)                                            \n",
    "ax.scatter(xcord, ycord, s = 20, c = 'blue',alpha = .5)                \n",
    "plt.title('DataSet')                                                \n",
    "plt.xlabel('X')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, this figure above looks very much similar to the one we just analysed. But if we look more closely at the y-axis, we will find that the y-values are 100 times the magnitude of the old one. Would it have any effect on our forecast? Let's try to build a tree to this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mat = np.mat(dataSet)\n",
    "print(createTree(data_mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that our tree hat a lot more leaf nodes, but the data set just shows two groups to be split. So we are having overfitting problem here. Let's try to modify our stopping conditions by changing the threshould value of error reduction (tolS) and build the tree again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(createTree(data_mat, ops=(10000,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After changing the value of tolS to 10000, we get the expected tree with two leaf nodes again. But we should not have to mess around with the stopping conditions to give us the tree we are looking for. Most of the time we have no idea how the tree should look like and we expect the machine to give us the big picture. So let's focus more on the *postpruning*, which uses a test set to prune the tree. This is a more idealistic method of pruning because it doesn't use any user-defined parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postpruning\n",
    "\n",
    "The method we use will first split our data into a test set and a training set. After that we need a funciton to check whether we are dealing with a tree or a leaf node. We shall have another function to calulate and compare the erros with or without merging two leaf nodes. If merging the nodes will reduce the error on the test set, we will implement a third funtion to merge them. Pseudo-code for *prune()* would look like this:\n",
    ">_Split the test data for the given tree:_  \n",
    "$\\quad$_If the either split is a tree: call prune on that split_  \n",
    "$\\quad$_Calculate the error associated with merging two leaf nodes_  \n",
    "$\\quad$_Calculate the error without merging_  \n",
    "$\\quad$_If merging results in lower error then merge the leaf nodes_ \n",
    "\n",
    "Let's take a look at the code for these functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isTree(obj):      # check whether it is a tree or a leaf node\n",
    "    return (type(obj).__name__ == 'dict') \n",
    " \n",
    "def getMean(tree):    # descend a tree untill it hits only leaf nodes, then take the mean value of both\n",
    "    if isTree(tree['right']): \n",
    "        tree['right'] = getMean(tree['right'])\n",
    "    if isTree(tree['left']): \n",
    "        tree['left'] = getMean(tree['left'])\n",
    "    return (tree['left'] + tree['right']) / 2.0    \n",
    "\n",
    "def prune(tree, testData):\n",
    "    if np.shape(testData)[0] == 0:    # return mean value of left and right nodes if no test data\n",
    "        return getMean(tree)  \n",
    "    if (isTree(tree['right']) or isTree(tree['left'])):  # split test data according to the trained tree\n",
    "        lSet, rSet = binSplitDataSet(testData, tree['spInd'], tree['spVal'])\n",
    "    if isTree(tree['left']): \n",
    "        tree['left'] = prune(tree['left'], lSet)      # prune the left subtree\n",
    "    if isTree(tree['right']): \n",
    "        tree['right'] = prune(tree['right'], rSet)    # prune the right subtree\n",
    "    if not isTree(tree['left']) and not isTree(tree['right']):     # if both branches are not trees\n",
    "        lSet, rSet = binSplitDataSet(testData, tree['spInd'], tree['spVal']) \n",
    "        errorNoMerge = np.sum(np.power(lSet[:,-1] - tree['left'],2)) + np.sum(np.power(rSet[:,-1] - tree['right'],2))\n",
    "\n",
    "        treeMean = (tree['left'] + tree['right']) / 2.0\n",
    "        errorMerge = np.sum(np.power(testData[:,-1] - treeMean, 2))\n",
    "        \n",
    "        if errorMerge < errorNoMerge: \n",
    "            print(\"merging\")\n",
    "            return treeMean\n",
    "        else: \n",
    "            return tree\n",
    "    else: \n",
    "        return tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to build the tree again to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before Pruning:\")\n",
    "dataSet = loadDataSet(\"./input/data3.txt\")\n",
    "data_mat = np.mat(dataSet)\n",
    "prePruningTree = createTree(data_mat, ops=(0,1))      # create the largest possible tree with ops(0,1)\n",
    "print(prePruningTree)\n",
    "print(\"\\nAfter Pruning:\")\n",
    "testSet = loadDataSet(\"./input/data3test.txt\") \n",
    "test_mat = np.mat(testSet)\n",
    "postPruningTree = prune(prePruningTree,test_mat)\n",
    "print(postPruningTree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm a big reduction of nodes after post-pruning. But the optimized tree still has more than  two nodes as we had hoped. It is recommmanded to use prepruning and postpruning to get the best possbile model. So far we have learned how to build and optimize a *regression tree* that contains a single value for each leaf node. In the next section we will explore a more advanced algorithm called *model tree*, where we will build a linear model at each leaf node instead of using mean values as in regression trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model trees\n",
    "\n",
    "An alternative to modeling the data as a simple constant value at each leaf node is to model it as a *piecewise linear model* at each leaf node. Piecewise linear means that we have a model that consists of multiple linear segments. Again let's take a look at a new data set first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSet = loadDataSet(\"./input/data4.txt\")\n",
    "n = len(dataSet)                                                    \n",
    "xcord = []; ycord = []                                                \n",
    "for i in range(n):                                                    \n",
    "    xcord.append(dataSet[i][0]); ycord.append(dataSet[i][1])        \n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)                                            \n",
    "ax.scatter(xcord, ycord, s = 20, c = 'blue',alpha = .5)                \n",
    "plt.title('DataSet')                                                \n",
    "plt.xlabel('X')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure above shows a dataset which mostly consists of two lines. So in this case it is better to model this as two straight lines than as a bunch of constant values. The first linear model should go from 0.0 to 0.3 and the second from 0.3 to 1.0. In order to do this, we will use the tree-generating algorithm to break up the data into segments that can easily be represented by a linear model, then we can use the functions for *linear regression* to generate the linear models at the leaf nodes instead of constant values. \n",
    "By measuring the error and determine the best split, We will first fit a linear model to the data and then measure how much our prediction differs from the target. This error is then squared and summed. Let's write some code to implement this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearSolve(dataSet):  # format the dataset into the target variable Y and the independent variable X\n",
    "    m,n = np.shape(dataSet)\n",
    "    X = np.mat(np.ones((m,n))); Y = np.mat(np.ones((m,1)))      # generate ones-matrix\n",
    "    X[:,1:n] = dataSet[:,0:n-1];                    # the 0th-column of matrix X is constant 1\n",
    "    Y = dataSet[:,-1]  \n",
    "    xTx = X.T*X\n",
    "    if np.linalg.det(xTx) == 0.0:                   # check the inversability\n",
    "        raise NameError('This matrix is singular, cannot do inverse,\\n\\\n",
    "        try increasing the second value of ops')\n",
    "    ws = xTx.I * (X.T * Y)                          # calculate the optimal weight matrix ws with least-squares method\n",
    "    return ws,X,Y\n",
    "\n",
    "def modelLeaf(dataSet):                             # generate a model for a leaf node\n",
    "    ws,X,Y = linearSolve(dataSet)\n",
    "    return ws\n",
    "\n",
    "def modelErr(dataSet):                              # calculate the total squared error \n",
    "    ws,X,Y = linearSolve(dataSet)                   # of model against target\n",
    "    yHat = X * ws\n",
    "    return sum(np.power(Y - yHat, 2))            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the model tree, we introduce new funcitons *modelLeaf()* and *modelErr()* to replace *regLeaf()* and *regErr()* for regression tree. Since we have all the functions needed to create a model tree, let's try them out on the new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myMat = np.mat(dataSet)\n",
    "myTree = createTree(myMat, modelLeaf, modelErr)  #create model tree\n",
    "print(myTree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result we find that the whole dataset was split by 0.285477. The linear models given by the algorithm are approximately:  \n",
    "$$ y = 0 + 11.96*x\\;\\;\\;and\\;\\;\\; y = 3.47 + 1.20*x $$  \n",
    "Let's plot this model tree and see whether it matches the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)     \n",
    "x_1 = np.linspace(0, 0.285477, 100)\n",
    "x_2 = np.linspace(0.285477, 1.0, 100)\n",
    "ax.plot(x_1, 3.47 + 1.20*x_1, c = 'red')                 \n",
    "ax.plot(x_2, 0 + 11.96*x_2, c = 'red')                   \n",
    "ax.scatter(xcord, ycord, s = 20, c = 'blue',alpha = .5) \n",
    "plt.title('DataSet')                                                \n",
    "plt.xlabel('X')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the figure above we can see that the dataset was perfectly represented by the model tree with two linear lines. So far we have learned *regression tree*, *model tree* and *standard linear regression*. To find out which one works the best for a given dataset, we can check the *correlation coefficient*, sometimes called the $R^2$ value by the function *corrcoef(yHat,y,rowvar=0)*, where *yHat* is the predicted values and *y* is the actual value of the target variable.  \n",
    "Next, let's compare the standard linear regression and tree-based methods in an example by using *corrcoef()* to see which one is the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree regression  vs. Standand regression\n",
    "\n",
    "In the section we will test our models on some data relating a person's intelligence with the number of speeds on their bicycle. The data used in this example is purely fictional. Let's load the training set and test set first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSet = loadDataSet(\"./input/bikeSpeedVsIq_train.txt\")\n",
    "testSet = loadDataSet(\"./input/bikeSpeedVsIq_test.txt\")\n",
    "trainMat = np.mat(trainingSet)\n",
    "testMat = np.mat(testSet)\n",
    "\n",
    "n = len(trainingSet)                                                    \n",
    "xcord = []; ycord = []                                                \n",
    "for i in range(n):                                                    \n",
    "    xcord.append(trainingSet[i][0]); ycord.append(trainingSet[i][1])        \n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)                                            \n",
    "ax.scatter(xcord, ycord, s = 20, c = 'blue',alpha = .5)                \n",
    "plt.title('DataSet')                                                \n",
    "plt.xlabel('X')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to write some functions to give us a forecast, for a given input and a given tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regTreeEval(model, inDat):     # evaluate a regression tree leaf node\n",
    "    return float(model)            # return the value at the leaf node\n",
    "\n",
    "def modelTreeEval(model, inDat):   # evaluate a model tree leaf node\n",
    "    n = np.shape(inDat)[1]\n",
    "    X = np.mat(np.ones((1, n+1)))\n",
    "    X[:, 1: n+1] = inDat\n",
    "    return float(X * model)        # return the forecasted value\n",
    "\n",
    "# give one forecast for one data point, for a given tree.\n",
    "def treeForeCast(tree, inData, modelEval=regTreeEval):  \n",
    "    if not isTree(tree):                                # when a leaf node is hit, run modelEval()\n",
    "        return modelEval(tree, inData)\n",
    "    \n",
    "    if inData[tree['spInd']] <= tree['spVal']:          # follow the tree based on the input data \n",
    "        if isTree(tree['left']):                        # until a leaf node is hit \n",
    "            return treeForeCast(tree['left'], inData, modelEval)\n",
    "        else:\n",
    "            return modelEval(tree['left'], inData)\n",
    "    else:\n",
    "        if isTree(tree['right']):\n",
    "            return treeForeCast(tree['right'], inData, modelEval)\n",
    "        else:\n",
    "            return modelEval(tree['right'], inData)\n",
    "        \n",
    "def createForeCast(tree, testData, modelEval=regTreeEval):\n",
    "    m = len(testData)\n",
    "    yHat = np.mat(np.zeros((m, 1)))\n",
    "    for i in range(m):\n",
    "        yHat[i] = treeForeCast(tree, np.mat(testData[i]), modelEval)\n",
    "    return yHat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build three models for the training dataset. We will start with regression tree and model tree models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myTree=createTree(trainMat, ops=(1,20))        # build regression tree\n",
    "yHat = createForeCast(myTree, testMat[:,0])\n",
    "print(\"correlation coefficients of regression tree: \", np.corrcoef(yHat, testMat[:,1], rowvar=0)[0,1])\n",
    "myTree2=createTree(trainMat, modelLeaf, modelErr, ops=(1, 20))     # build model tree\n",
    "yHat2 = createForeCast(myTree2, testMat[:, 0], modelTreeEval)\n",
    "print(\"correlation coefficients of model tree: \", np.corrcoef(yHat2, testMat[:, 1],rowvar=0)[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both values are very close to each other. But still the model tree shows a better performance since its value is closer to the 1.0. Now let’s see it for a standard linear regression with the help of function *linearSolve()*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws,X,Y=linearSolve(trainMat)     # calculate the weights for linear regression\n",
    "ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the  yHat values,we will loop over the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(np.shape(testMat)[0]):\n",
    "    yHat[i]=testMat[i,0]*ws[1,0]+ws[0,0]\n",
    "print(\"correlation coefficients of standard linear regression: \", np.corrcoef(yHat, testMat[:, 1],rowvar=0)[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $R^2$ value is lower than the other two methods, which means that the tree models work better at predicting complex data than a simple linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Regression is the process of predicting a target value, which makes it become one of the most useful tools in statistics. The *standard linear regression* does a good job, when the dataset is simple and linear. By using the *locally weighted linear regression*, the forecast would be more precise. \n",
    "\n",
    "But oftertimes the data may contain complex interactions which lead to nonlinear relationships between the input and target variables. The method *regression tree* breaks up the predicted value into piecewise constant segments while the *model tree* implements the linear regression equations at each leaf node. Model trees and regression trees can be built with the *CART* algorithm as long as the right error measurements are chosen. *Prepruning* and *postpruning* can effectively reduce the complexity of the tree and help avoiding the overfitting problems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
