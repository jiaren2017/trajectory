{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#import regression_code as reg\n",
    "#import read_data as read\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.patches import ConnectionPatch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################## \n",
    "######     Import Package & Helper Function  #####\n",
    "################################################## \n",
    "\n",
    "def loadDataArr(fileName):\n",
    "    numFeat = len(open(fileName).readline().split('\\t')) - 1\n",
    "    xArr = []; yArr = []\n",
    "    fr = open(fileName)\n",
    "    for line in fr.readlines():\n",
    "        lineArr =[]\n",
    "        curLine = line.strip().split('\\t')\n",
    "        for i in range(numFeat):\n",
    "            lineArr.append(float(curLine[i]))\n",
    "        xArr.append(lineArr)\n",
    "        yArr.append(float(curLine[-1]))\n",
    "    return xArr, yArr\n",
    "\n",
    "def loadDataList(fileName):\n",
    "    dataList = []\n",
    "    fr = open(fileName)\n",
    "    for line in fr.readlines():\n",
    "        curLine = line.strip().split('\\t')\n",
    "        fltLine = list(map(float, curLine))      # map data to float()\n",
    "        dataList.append(fltLine)\n",
    "    return dataList\n",
    "\n",
    "############################################# \n",
    "######     Standard linear regression  #####\n",
    "############################################# \n",
    "    \n",
    "'''   old functions with array as input and output\n",
    "def stdLinReg (xArr_train, yArr_train):                                  # calculate the optimal weights for regression\n",
    "    xMat_train = np.mat(xArr_train); yMat_train = np.mat(yArr_train).T               # convert array to matrix\n",
    "    xTx = xMat_train.T * xMat_train                            \n",
    "    if np.linalg.det(xTx) == 0.0:                                        # test the inversability\n",
    "        print(\"This matrix is singular, can not do inverse\")\n",
    "        return\n",
    "    wsMat_stdLinReg = xTx.I * (xMat_train.T*yMat_train)\n",
    "    return wsMat_stdLinReg\n",
    "    \n",
    "def pred_stdLinReg(xArr_test, wsMat_stdLinReg):\n",
    "    xMat = np.mat(xArr_test);                                  # convert array to matrix\n",
    "    xCopy = xMat.copy(); xCopy.sort(0)                         # sort the points in ascending order for pyplot\n",
    "    yMat_predOrdered = xCopy * wsMat_stdLinReg\n",
    "    yMat_predNormal = xMat * wsMat_stdLinReg\n",
    "    return yMat_predNormal, yMat_predOrdered\n",
    "    \n",
    "\n",
    "def showStdLinReg(xArr_test, yArr_test, yMat_predOrdered=None):\n",
    "    xMat = np.mat(xArr_test); yMat = np.mat(yArr_test)         # convert array to matrix\n",
    "\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "    ax = fig.add_subplot(111)                                  # create subplot\n",
    "    if (np.shape(xMat)[1] == 1):                               # if there is 1 feature column\n",
    "        ax.scatter(xMat[:,0].flatten().A[0], yMat.T[:,0].flatten().A[0], s=20, c='blue', alpha=.5)     # plot original data\n",
    "    elif(np.shape(xMat)[1] == 2):                              # if there are 2 feature columns\n",
    "        ax.scatter(xMat[:,1].flatten().A[0], yMat.T[:,0].flatten().A[0], s=20, c='blue', alpha=.5)     # plot original data\n",
    "    \n",
    "    if (np.all(yMat_predOrdered != None)):\n",
    "        xCopy = xMat.copy(); xCopy.sort(0)                         # sort the points in ascending order for pyplot  \n",
    "        if (np.shape(xMat)[1] == 1):\n",
    "            ax.plot(xCopy[:,0], yMat_predOrdered, c='red')             # show regression line\n",
    "        elif(np.shape(xMat)[1] == 2): \n",
    "            ax.plot(xCopy[:,1], yMat_predOrdered, c='red')             # show regression line\n",
    "        plt.title('Linear Regression')                             # draw title    \n",
    "    \n",
    "    plt.xlabel('X')\n",
    "    plt.show()\n",
    "'''\n",
    "\n",
    "def stdLinReg (dataMat):                    # format the dataset into the target variable Y and the independent variable X\n",
    "    m,n = np.shape(dataMat)\n",
    "    if n == 2:    # only 1D-x and y:  there are only one feature for x, namely no offset column, then add one for the linear regression\n",
    "        X = np.mat(np.ones((m,n)))          # generate ones-matrix\n",
    "        X[:,1:n] = dataMat[:,0:n-1]       # copy feature matrix to X, the 0th-column of matrix X is constant 1 as offset\n",
    "    elif n>2:   # if there are more than one feature for X\n",
    "        X = np.mat(np.ones((m,n-1)))        # generate ones-matrix\n",
    "        X[:,0:n-1] = dataMat[:,0:n-1]       # copy feature matrix to X\n",
    "    else:\n",
    "        raise NameError('no valid input matrix')\n",
    "    Y = np.mat(np.ones((m,1)))        \n",
    "    Y = dataMat[:,-1]                  # copy target matrix to Y\n",
    "    xTx = X.T*X\n",
    "    if np.linalg.det(xTx) == 0.0:                   # check the inversability\n",
    "        raise NameError('This matrix is singular, cannot do inverse,\\n\\\n",
    "        try increasing the second value of Stop Condition')\n",
    "    ws = xTx.I * (X.T * Y)                          # calculate the optimal weight matrix ws with least-squares method\n",
    "    return ws,X,Y\n",
    "\n",
    "\n",
    "def pred_stdLinReg(dataMat_test, wsMat_stdLinReg):\n",
    "    m,n = np.shape(dataMat_test)\n",
    "    if n == 2:    # only 1D-x and y:  there are only one feature for x, namely no offset column, then add one for the linear regression\n",
    "        X = np.mat(np.ones((m,n)))       # generate ones-matrix\n",
    "        X[:,1:n] = dataMat_test[:,0:n-1]       # copy feature matrix to X, the 0th-column of matrix X is constant 1 as offset\n",
    "    elif n>2:   # if there are more than one feature for X\n",
    "        X = np.mat(np.ones((m,n-1)))      # generate ones-matrix\n",
    "        X[:,0:n-1] = dataMat_test[:,0:n-1]      # copy feature matrix to X\n",
    "    else:\n",
    "        raise NameError('no valid input matrix')\n",
    "    \n",
    "    yMat_pred = X * wsMat_stdLinReg\n",
    "    return yMat_pred\n",
    "\n",
    "\n",
    "def showStdLinReg(dataList, yMat_pred=None):\n",
    "    n = len(dataList)                                                    \n",
    "    xcord = []; ycord = []      \n",
    "\n",
    "    if (np.shape(dataList)[1] == 2):            # check the number of columns\n",
    "        for i in range(n):   \n",
    "            xcord.append(dataList[i][0]); ycord.append(dataList[i][1])     # no offset in datalist, so use the first column as x\n",
    "    elif(np.shape(dataList)[1] == 3):           \n",
    "        for i in range(n):   \n",
    "            xcord.append(dataList[i][1]); ycord.append(dataList[i][2])     # the first conlumn is offset, so use the second column as x\n",
    "\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "    ax = fig.add_subplot(111)                                            \n",
    "    ax.scatter(xcord, ycord, s = 20, c = 'blue',alpha = .5)      # plot the orignial data set\n",
    "    if(np.all(yMat_pred != None)):\n",
    "        yArr_pred = yMat_pred[:,0].flatten().A[0]                # convert matrix to array\n",
    "        xMat_test = np.mat(xcord).T          \n",
    "        srtInd = xMat_test.argsort(0)    \n",
    "        xSort  = xMat_test[srtInd][:,0,:]                       # copy the xMat_test in ascending order for pyplot\n",
    "        ax.plot(xSort[:], yArr_pred[srtInd], c = 'red')         # plot the prediction\n",
    "        plt.title('Linear Regression')                             # draw title    \n",
    "    plt.xlabel('X')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def calError(yArr_actual, yArr_pred):                          # calculate the squared error\n",
    "    return ((yArr_actual - yArr_pred) **2).sum() \n",
    "    \n",
    "    \n",
    "    \n",
    "##################################################### \n",
    "######     Locally weighted linear regression  #####   \n",
    "##################################################### \n",
    "\n",
    "'''\n",
    "def localWeightLinReg(testPoint, xArr_train, yArr_train, k = 1.0):       # run lwlr-prediction  on a SINGLE value\n",
    "    xMat_train = np.mat(xArr_train); yMat_train = np.mat(yArr_train).T             # convert array to matrix\n",
    "    m = np.shape(xMat_train)[0]\n",
    "    weights = np.mat(np.eye((m)))                          # Create diagonal matrix of weights\n",
    "    for j in range(m):                                     # Populate weights with exponentially decaying values\n",
    "        diffMat = testPoint - xMat_train[j, :]                                 \n",
    "        weights[j, j] = np.exp(diffMat * diffMat.T/(-2.0 * k**2))     # locally weighted\n",
    "    xTx = xMat_train.T * (weights * xMat_train)                                        \n",
    "    if np.linalg.det(xTx) == 0.0:                                     # test the inversability\n",
    "        print(\"This matrix is singular, cannot do inverse\")\n",
    "        return\n",
    "    ws = xTx.I * (xMat_train.T * (weights * yMat_train))               # calculate the weights for \"Locally weighted linear regression\"\n",
    "    return testPoint * ws                                              # return the predicted single value\n",
    "\n",
    "def lwlr_Test(xArr_test, xArr_train, yArr_train, k=1.0):     # run lwlr-prediction  on a test ARRAY\n",
    "    m = np.shape(xArr_test)[0]                               # check the size of data set\n",
    "    yArr_pred = np.zeros(m)    \n",
    "    for i in range(m):                                       # predict for each single element of test array\n",
    "        yArr_pred[i] = localWeightLinReg(xArr_test[i], xArr_train, yArr_train, k)\n",
    "    return yArr_pred\n",
    "    \n",
    "def showLwlr(xArr_test, yArr_test, yArr_pred=None, k=1.0):\n",
    "    xMat_test = np.mat(xArr_test); yMat_test = np.mat(yArr_test)             # convert array to matrix\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "    ax = fig.add_subplot(111)                                 # create subplot    \n",
    "    if (np.shape(xMat_test)[1] == 1):\n",
    "        ax.scatter(xMat_test[:,0].flatten().A[0], yMat_test.T[:,0].flatten().A[0], s=20, c='blue', alpha=.5)     # plot original data\n",
    "    elif(np.shape(xMat_test)[1] == 2):  \n",
    "        ax.scatter(xMat_test[:,1].flatten().A[0], yMat_test.T[:,0].flatten().A[0], s=20, c='blue', alpha=.5)     # plot original data\n",
    "    \n",
    "    if(np.all(yArr_pred != None)):\n",
    "        if (np.shape(xMat_test)[1] == 1):\n",
    "            srtInd = xMat_test[:,0].argsort(0)                        # return the ordered index according to the value of xMat[:,1]\n",
    "            xSort  = xMat_test[srtInd][:,0,:]                         # copy the xMat in ascending order for pyplot\n",
    "            ax.plot(xSort[:, 0], yArr_pred[srtInd], c = 'red')                  # plot the regression line\n",
    "        elif(np.shape(xMat_test)[1] == 2): \n",
    "            srtInd = xMat_test[:,1].argsort(0)                        # return the ordered index according to the value of xMat[:,1]\n",
    "            xSort  = xMat_test[srtInd][:,0,:]                         # copy the xMat in ascending order for pyplot\n",
    "            ax.plot(xSort[:, 1], yArr_pred[srtInd], c = 'red')                  # plot the regression line\n",
    "            plt.title('Locally weighted linear regression, k={}'.format(k))     # draw title    \n",
    "\n",
    "    plt.xlabel('X')\n",
    "    plt.show()\n",
    "'''\n",
    "def localWeightLinReg(testPoint, dataMat_train, k = 1.0):       # run lwlr-prediction  on a SINGLE value\n",
    "    m,n = np.shape(dataMat_train)\n",
    "    if n == 2:    # only 1D-x and y:  there are only one feature for x, namely no offset column, then add one for the linear regression\n",
    "        XMat_train = np.mat(np.ones((m,n)))          # generate ones-matrix\n",
    "        XMat_train[:,1:n] = dataMat_train[:,0:n-1]       # copy feature matrix to X, the 0th-column of matrix X is constant 1 as offset\n",
    "    elif n>2:   # if there are more than one feature for X\n",
    "        XMat_train = np.mat(np.ones((m,n-1)))        # generate ones-matrix\n",
    "        XMat_train[:,0:n-1] = dataMat_train[:,0:n-1]       # copy feature matrix to X\n",
    "    else:\n",
    "        raise NameError('no valid input matrix')\n",
    "    YMat_train = np.mat(np.ones((m,1)))        \n",
    "    YMat_train = dataMat_train[:,-1]                  # copy target matrix to Y\n",
    "    \n",
    "    weights = np.mat(np.eye((m)))                          # Create diagonal matrix of weights\n",
    "    for j in range(m):                                     # Populate weights with exponentially decaying values\n",
    "        diffMat = testPoint - XMat_train[j,:]                                 \n",
    "        weights[j, j] = np.exp(diffMat * diffMat.T/(-2.0 * k**2))     # locally weighted     \n",
    "    xTx = XMat_train.T * (weights * XMat_train)                                        \n",
    "    if np.linalg.det(xTx) == 0.0:                                     # test the inversability\n",
    "        print(\"This matrix is singular, cannot do inverse\")\n",
    "        return\n",
    "    ws = xTx.I * (XMat_train.T * (weights * YMat_train))               # calculate the weights for \"Locally weighted linear regression\"\n",
    "    return testPoint * ws   \n",
    "    \n",
    "\n",
    "    \n",
    "def lwlr_Test(dataMat_test, dataMat_train, k=1.0):     # run lwlr-prediction  on a test ARRAY\n",
    "    m,n = np.shape(dataMat_test)\n",
    "    if n == 2:                                      # only 1D-x and y:  there are only one feature for x, namely no offset column, then add one for the linear regression\n",
    "        Xmat_test = np.mat(np.ones((m,n)))          # generate ones-matrix\n",
    "        Xmat_test[:,1:n] = dataMat_test[:,0:n-1]    # copy feature matrix to X, the 0th-column of matrix X is constant 1 as offset\n",
    "    elif n>2:                                       # if there are more than one feature for X\n",
    "        Xmat_test = np.mat(np.ones((m,n-1)))        # generate ones-matrix\n",
    "        Xmat_test[:,0:n-1] = dataMat_test[:,0:n-1]       # copy feature matrix to X\n",
    "    else:\n",
    "        raise NameError('no valid input matrix')\n",
    "    \n",
    "    yArr_pred = np.zeros(m)    \n",
    "    for i in range(m):                                       # predict for each single element of test array\n",
    "        yArr_pred[i] = localWeightLinReg(Xmat_test[i], dataMat_train, k)\n",
    "    return yArr_pred\n",
    "\n",
    "\n",
    "def showLwlr(dataList, yArr_pred=None, k=1.0):\n",
    "    n = len(dataList)                                                    \n",
    "    xcord = []; ycord = []      \n",
    "\n",
    "    if (np.shape(dataList)[1] == 2):            # check the number of columns\n",
    "        for i in range(n):   \n",
    "            xcord.append(dataList[i][0]); ycord.append(dataList[i][1])     # no offset in datalist, so use the first column as x\n",
    "    elif(np.shape(dataList)[1] == 3):           \n",
    "        for i in range(n):   \n",
    "            xcord.append(dataList[i][1]); ycord.append(dataList[i][2])     # the first conlumn is offset, so use the second column as x\n",
    "\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "    ax = fig.add_subplot(111)                                 # create subplot   \n",
    "    ax.scatter(xcord, ycord, s = 20, c = 'blue',alpha = .5)      # plot the orignial data set\n",
    "    if(np.all(yArr_pred != None)):\n",
    "        xMat_test = np.mat(xcord).T          \n",
    "        srtInd = xMat_test.argsort(0)    \n",
    "        xSort  = xMat_test[srtInd][:,0,:]                       # copy the xMat_test in ascending order for pyplot\n",
    "        ax.plot(xSort[:], yArr_pred[srtInd], c = 'red')         # plot the prediction\n",
    "        plt.title('Locally weighted linear regression, k={}'.format(k))     # draw title    \n",
    "    plt.xlabel('X')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "########################################################### \n",
    "###############    Tree-based Methods    ###############\n",
    "###########################################################\n",
    "\n",
    "\n",
    "\n",
    "def showTree(dataList, yArr_pred=None, mode=\"regTree\"):\n",
    "    n = len(dataList)                                                    \n",
    "    xcord = []; ycord = []      \n",
    "\n",
    "    if (np.shape(dataList)[1] == 2):            # check the number of columns\n",
    "        for i in range(n):   \n",
    "            xcord.append(dataList[i][0]); ycord.append(dataList[i][1])    \n",
    "    elif(np.shape(dataList)[1] == 3):\n",
    "        for i in range(n):   \n",
    "            xcord.append(dataList[i][1]); ycord.append(dataList[i][2])    \n",
    "\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "    ax = fig.add_subplot(111)                                            \n",
    "    ax.scatter(xcord, ycord, s = 20, c = 'blue',alpha = .5)      # plot the orignial data set\n",
    "    if(np.all(yArr_pred != None)):\n",
    "        xMat_test = np.mat(xcord).T          \n",
    "        srtInd = xMat_test.argsort(0)    \n",
    "        xSort  = xMat_test[srtInd][:,0,:]                       # copy the xMat_test in ascending order for pyplot\n",
    "        ax.plot(xSort[:], yArr_pred[srtInd], c = 'red')         # plot the prediction\n",
    "    if(mode == \"regTree\"):\n",
    "        plt.title('Regression  Tree')         \n",
    "    elif(mode == \"modTree\"):\n",
    "        plt.title('Model  Tree')  \n",
    "    plt.xlabel('X')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "##########################   functions for building Regression Tree    ##########################\n",
    "\n",
    "def regLeaf(dataMat):                      # calculate the MEAN value as the model for a Leaf node\n",
    "    return np.mean(dataMat[:,-1])          # dataMat[:,-1]: the last column of dataMat is Y\n",
    "\n",
    "def regErr(dataMat):                       # calculate the TOTAL Squared Error of the target variables in a given dataset\n",
    "    return np.var(dataMat[:,-1]) * np.shape(dataMat)[0]            # var(x): mean((x_i - x.mean())**2)\n",
    "    # the smaller the variance is, the better the split. Goal: try to use LEAST split to seperate the whole data set\n",
    "\n",
    "##########################   functions for building Model Tree    ##############################\n",
    "\n",
    "def linearSolve(dataMat):  # format the dataset into the target variable Y and the independent variable X\n",
    "    m,n = np.shape(dataMat)\n",
    "    X = np.mat(np.ones((m,n))); Y = np.mat(np.ones((m,1)))      # generate ones-matrix\n",
    "    X[:,1:n] = dataMat[:,0:n-1];       # copy feature matrix to X, the 0th-column of matrix X is constant 1 as offset\n",
    "    Y = dataMat[:,-1]                  # copy target matrix to Y\n",
    "    xTx = X.T*X\n",
    "    if np.linalg.det(xTx) == 0.0:                   # check the inversability\n",
    "        raise NameError('This matrix is singular, cannot do inverse,\\n\\\n",
    "        try increasing the second value of Stop Condition')\n",
    "    ws = xTx.I * (X.T * Y)                          # calculate the optimal weight matrix ws with least-squares method\n",
    "    return ws,X,Y\n",
    "\n",
    "\n",
    "def modelLeaf(dataMat):                             # generate a model for a leaf node\n",
    "    ws,X,Y = linearSolve(dataMat)\n",
    "    return ws\n",
    "\n",
    "def modelErr(dataMat):                              # calculate the total squared error \n",
    "    ws,X,Y = linearSolve(dataMat)                   # of model against target\n",
    "    yHat = X * ws\n",
    "    return sum(np.power(Y - yHat, 2))        \n",
    "\n",
    "\n",
    "##########################   functions for split    ##############################\n",
    "\n",
    "def binSplitDataSet(dataMat, feature, value):                          # binary split\n",
    "    mat0 = dataMat[np.nonzero(dataMat[:,feature] <= value)[0],:]       # np.nonzero(dataMat[:,feature] <= value)[0]: return index of target rows\n",
    "    mat1 = dataMat[np.nonzero(dataMat[:,feature] > value)[0],:]        # np.nonzero(dataMat[:,feature] > value)[0]: return index of target rows\n",
    "    return mat0, mat1\n",
    "\n",
    "def chooseBestSplit(dataMat, leafType = regLeaf, errType = regErr, stopCond = (1,4)):\n",
    "    minErrReduction = stopCond[0];         # stop condition: minimal Error reduction should be made through a new split\n",
    "    minInstance = stopCond[1]              # stop condition: minimal amount of instances should be included in a leaf node\n",
    "    if len(set(dataMat[:,-1].T.tolist()[0])) == 1:     # If all y-values are equal, NO SPLIT: Leaf node\n",
    "        return None, leafType(dataMat)                 # calculate value for leaf node  \n",
    "    \n",
    "    m, n = np.shape(dataMat)      # get the size of dataset\n",
    "    preError = errType(dataMat)   # setting the last feature as the best split and estimate its error for further compare\n",
    "    bestError = float('inf');     # initialize bestError as an infinite value\n",
    "    bestIndex = 0;                # initialize best splitting feature(Index) \n",
    "    bestValue = 0                 # initialize best splitting value\n",
    "  \n",
    "    for featIndex in range(n - 1):   # iterate all feature columns to find the splitting feature and splitting value\n",
    "        for splitVal in set(dataMat[:,featIndex].T.tolist()[0]):    # iterate all x-values of ONE certain feature\n",
    "            mat0, mat1 = binSplitDataSet(dataMat, featIndex, splitVal) \n",
    "            if (np.shape(mat0)[0] < minInstance) or (np.shape(mat1)[0] < minInstance): continue  # stop conditions met, NO SPLIT: Leaf node\n",
    "            newError = errType(mat0) + errType(mat1)      # calculate the new error from two split sets\n",
    "            if newError < bestError:                      # update if new error is smaller than best error\n",
    "                bestIndex = featIndex\n",
    "                bestValue = splitVal\n",
    "                bestError = newError\n",
    "                \n",
    "    if (preError - bestError) < minErrReduction:                 # If stop conditions met, NO SPLIT: leaf node\n",
    "        return None, leafType(dataMat)                           # calculate value for leaf node\n",
    "    \n",
    "    mat0, mat1 = binSplitDataSet(dataMat, bestIndex, bestValue)   # otherweise make the best split\n",
    "    if (np.shape(mat0)[0] < minInstance) or (np.shape(mat1)[0] < minInstance):  # If stop conditions met, NO SPLIT: leaf node\n",
    "        return None, leafType(dataMat)                            # calculate value for leaf node  \n",
    "    return bestIndex, bestValue\n",
    "\n",
    "\n",
    "##########################   functions for Creating and Pruning tree    ##############################\n",
    "\n",
    "def createTree(dataMat_train, leafType = regLeaf, errType = regErr, stopCond = (1, 4)):\n",
    "    feat, val = chooseBestSplit(dataMat_train, leafType, errType, stopCond)\n",
    "    if feat == None: return val        # If stop condition met, return leaf value for the leaf node \n",
    "    retTree = {}                       # define retTree as dictionary\n",
    "    retTree['spFeatIndex'] = feat\n",
    "    retTree['spValue'] = val\n",
    "    left_Set, right_Set = binSplitDataSet(dataMat_train, feat, val)\n",
    "    retTree['left'] = createTree(left_Set, leafType, errType, stopCond)\n",
    "    retTree['right'] = createTree(right_Set, leafType, errType, stopCond)\n",
    "    return retTree  \n",
    "\n",
    "def isTree(obj):      # check whether it is a tree or a leaf node\n",
    "    return (type(obj).__name__ == 'dict') \n",
    " \n",
    "\n",
    "def getMean(tree):    # descend a tree untill it hits only leaf nodes, then take the MEAN value of both\n",
    "    if isTree(tree['right']): \n",
    "        tree['right'] = getMean(tree['right'])\n",
    "    if isTree(tree['left']): \n",
    "        tree['left'] = getMean(tree['left'])\n",
    "    return (tree['left'] + tree['right']) / 2.0    \n",
    "\n",
    "\n",
    "def prune(tree, testData):            # Post-pruning\n",
    "    if np.shape(testData)[0] == 0:    # If no test data return MEAN value of left and right nodes \n",
    "        return getMean(tree)  \n",
    "    \n",
    "    if (isTree(tree['right']) or isTree(tree['left'])):  # split test data according to the trained tree\n",
    "        lSet, rSet = binSplitDataSet(testData, tree['spFeatIndex'], tree['spValue'])\n",
    "    if isTree(tree['left']): \n",
    "        tree['left'] = prune(tree['left'], lSet)      # prune the left subtree\n",
    "    if isTree(tree['right']): \n",
    "        tree['right'] = prune(tree['right'], rSet)    # prune the right subtree\n",
    "    if not isTree(tree['left']) and not isTree(tree['right']):     # if the leaf node of trained tree is reached\n",
    "        \n",
    "        #  test the total squared error with the value of leaf node of trained tree\n",
    "        lSet, rSet = binSplitDataSet(testData, tree['spFeatIndex'], tree['spValue']) \n",
    "        errorNoMerge = np.sum(np.power(lSet[:,-1] - tree['left'], 2)) + np.sum(np.power(rSet[:,-1] - tree['right'], 2))\n",
    "\n",
    "        #  test the total squared error with the MEAN value of leaf node of trained tree\n",
    "        treeMean = (tree['left'] + tree['right']) / 2.0\n",
    "        errorMerge = np.sum(np.power(testData[:,-1] - treeMean, 2))\n",
    "        \n",
    "        if errorMerge < errorNoMerge: \n",
    "  #          print(\"merging, tree['spFeatIndex']: {}, tree['spValue']:{}, tree['left']:{}, tree['right']:{}\".format(tree['spFeatIndex'],tree['spValue'],tree['left'],tree['right']))\n",
    "  #          print(\"treeMean:\", treeMean)\n",
    "  #          print(\"\\n\")\n",
    "            return treeMean             # MERGE the left and right leaf node into one leaf node with MEAN value\n",
    "        else: \n",
    "            return tree\n",
    "    else: \n",
    "        return tree\n",
    "    \n",
    "    \n",
    "    \n",
    "##########################   functions for Prediction with Tree Model    ##############################\n",
    "\n",
    "def regTreeEval(model, inDat):     # evaluate a Regression Tree leaf node\n",
    "    return float(model)            # return the value at the leaf node\n",
    "\n",
    "\n",
    "def modelTreeEval(model, inDat):   # evaluate a Model Tree leaf node\n",
    "    n = np.shape(inDat)[1]\n",
    "    X = np.mat(np.ones((1, n+1)))  # n+1 features, including the offset\n",
    "    X[:, 1: n+1] = inDat           # copy inDat to X second to (n+1)th. column, X first column is offset with value '1'\n",
    "    return float(X * model)        # return the forecasted value\n",
    "\n",
    "\n",
    "# give one forecast for one data point, for a given tree.\n",
    "def treeForecast(tree_trained, dataMat_test, modelEval=regTreeEval):\n",
    "\n",
    "    if not isTree(tree_trained):                                # when a leaf node is hit, run modelEval()\n",
    "        return modelEval(tree_trained, dataMat_test)\n",
    "    \n",
    "    if dataMat_test[:,tree_trained['spFeatIndex']] <= tree_trained['spValue']:    # follow the tree based on the input data \n",
    "        if isTree(tree_trained['left']):                                          # until a leaf node is hit \n",
    "            return treeForecast(tree_trained['left'], dataMat_test, modelEval)\n",
    "        else:\n",
    "            return modelEval(tree_trained['left'], dataMat_test)\n",
    "    else:\n",
    "        if isTree(tree_trained['right']):\n",
    "            return treeForecast(tree_trained['right'], dataMat_test, modelEval)\n",
    "        else:\n",
    "            return modelEval(tree_trained['right'], dataMat_test)\n",
    "        \n",
    "'''\n",
    "def createForeCast(tree_trained, dataMat_test, modelEval=regTreeEval):\n",
    "    m = len(dataMat_test)\n",
    "    yArr_pred = np.zeros(m)\n",
    "    for i in range(m):                        #  run prediction for each SINGLE value of test set\n",
    "        yArr_pred[i] = treeForecast(tree_trained, np.mat(dataMat_test[i]), modelEval)\n",
    "    return yArr_pred\n",
    "'''\n",
    "def createForeCast(tree_trained, dataMat_test, modelEval=regTreeEval, iterate='false', treeX_trained=None, treeY_trained=None, iterNum=10, minDist=10):\n",
    "    if (iterate == 'false'):                      #  just run forecast based on the test data\n",
    "        m = len(dataMat_test)\n",
    "        yArr_pred = np.zeros(m)\n",
    "        for i in range(m):                        #  run prediction for each SINGLE value of test set\n",
    "            yArr_pred[i] = treeForecast(tree_trained, np.mat(dataMat_test[i]), modelEval)\n",
    "        return yArr_pred\n",
    "    \n",
    "    else:                                         #  iterate till the stop conditions met\n",
    "        dataMat_pred = np.mat(np.zeros((iterNum,6)))\n",
    "        dataMat_pred[0,0:4] = dataMat_test        # assign the position and speed of test data\n",
    "        for i in range(iterNum-1):\n",
    "            # calculate the acceleratex x(i), y(i)\n",
    "            dataMat_pred[i,4] = treeForecast(treeX_trained, dataMat_pred[i,0:4], modelEval)\n",
    "            dataMat_pred[i,5] = treeForecast(treeY_trained, dataMat_pred[i,0:4], modelEval)\n",
    "            \n",
    "            # calculate the NEW position x(i+1), y(i+1)\n",
    "            dataMat_pred[i+1,0] = dataMat_pred[i,0] + dataMat_pred[i,2] + dataMat_pred[i,4]\n",
    "            dataMat_pred[i+1,1] = dataMat_pred[i,1] + dataMat_pred[i,3] + dataMat_pred[i,5]\n",
    "            \n",
    "            # calculate the NEW speed x(i+1), y(i+1)\n",
    "            dataMat_pred[i+1,2] = dataMat_pred[i,2] + dataMat_pred[i,4]\n",
    "            dataMat_pred[i+1,3] = dataMat_pred[i,3] + dataMat_pred[i,5]\n",
    "        \n",
    "        dataMat_pred[-1,4] = treeForecast(treeX_trained, dataMat_pred[-1,0:4], modelEval)\n",
    "        dataMat_pred[-1,5] = treeForecast(treeY_trained, dataMat_pred[-1,0:4], modelEval)\n",
    "        \n",
    "        return dataMat_pred\n",
    "\n",
    "\n",
    "\n",
    "##########################   functions for plotting the tree    ##############################\n",
    "\n",
    "\n",
    "def getNumLeafs(tree, numLeafNode=0):\n",
    "   \n",
    "    if isTree(tree['left']):       # check the 'left' part, whether it is a leaf node already\n",
    "        numLeafNode = getNumLeafs(tree['left'], numLeafNode)\n",
    "    else:\n",
    "        numLeafNode += 1           # 'left' is a leaf node,then increment the total number of leaf node and then  check the 'right' of the SAME level!\n",
    "    if isTree(tree['right']):      # check the 'right' of the SAME level\n",
    "        numLeafNode = getNumLeafs(tree['right'], numLeafNode)\n",
    "    else:\n",
    "        return numLeafNode + 1     # if it is a lefe node, then return to the last stage\n",
    "    \n",
    "    return numLeafNode\n",
    "\n",
    "\n",
    "def getDepth(tree, numTreeDepth=0, max =0):\n",
    "    \n",
    "    if not isTree(tree): \n",
    "        return 0\n",
    "    if isTree(tree['left']):       # check the 'left' part, whether it is a tree \n",
    "        max = getDepth(tree['left'], numTreeDepth + 1, max)     # it is a tree, then go deep\n",
    "    if isTree(tree['right']):      # check the 'right' of the SAME level\n",
    "        max = getDepth(tree['right'], numTreeDepth + 1, max)\n",
    "    else:\n",
    "        numTreeDepth += 1\n",
    "    max = numTreeDepth if numTreeDepth >= max else max\n",
    "    return max             # return to the last stage\n",
    "    \n",
    "    \n",
    "def getTreeDepth(tree):\n",
    "    leftDepth = getDepth(tree['left'])\n",
    "    rightDepth = getDepth(tree['right'])\n",
    "    treeDepth = leftDepth if leftDepth >= rightDepth else rightDepth\n",
    "    return treeDepth+1              # plus the very first splitt\n",
    "\n",
    "###########################################################################################\n",
    "###########################################################################################\n",
    "\n",
    "\n",
    "def plotNode(nodeTxt, centerPt, parentPt, nodeType):                              # plot comment with arrow\n",
    "    arrow_args = dict(arrowstyle=\"<-\")                                            # set arrow format\n",
    "#    font = FontProperties(fname=r\"c:\\windows\\fonts\\simsun.ttc\", size=14)         # set chinese fond\n",
    "    createPlot.ax1.annotate(nodeTxt, xy=parentPt,  xycoords='axes fraction',      # plot node\n",
    "        xytext=centerPt, textcoords='axes fraction',\n",
    "        va=\"center\", ha=\"center\", bbox=nodeType, arrowprops=arrow_args)\n",
    "        \n",
    "\n",
    "def plotMidText(cntrPt, parentPt, txtString):                                     #   plot transfer information bewteen tree and subtree\n",
    "    xMid = (parentPt[0]-cntrPt[0])/2.0 + cntrPt[0]                                # calculate position                  \n",
    "    yMid = (parentPt[1]-cntrPt[1])/2.0 + cntrPt[1]\n",
    "    createPlot.ax1.text(xMid, yMid, txtString, va=\"center\", ha=\"center\", rotation=30)\n",
    "    \n",
    "###########################################################################################\n",
    "    \n",
    "def plotTree(myTree, parentPt, nodeTxt, factorX=1, factorY=1):\n",
    "    decisionNode = dict(boxstyle=\"sawtooth\", fc=\"0.8\")      # set decision node format，frame and arrow\n",
    "    leafNode = dict(boxstyle=\"round4\", fc=\"0.8\")            # set leaf node format\n",
    "    numLeafs = getNumLeafs(myTree)                          # get current number of total leaf nodes\n",
    "#    depth = getTreeDepth(myTree)                           # get depth of tree\n",
    "    cntrPt = (plotTree.xOff + (1.0 + float(numLeafs))/2.0/plotTree.totalW, plotTree.yOff)      #  define center position\n",
    "\n",
    "##### plot decision node by plotNode(nodeTxt, centerPt, parentPt, nodeType)\n",
    "    spFeatIndex = myTree['spFeatIndex']  \n",
    "    spValue =  round(myTree['spValue'],2)                     # get the splitting point\n",
    "    plotMidText(cntrPt, parentPt, '')\n",
    "    plotNode(\"Feat:\"+ str(spFeatIndex)+\"\\n\"+\"Val: \" + str(spValue), cntrPt,  parentPt, decisionNode) \n",
    "    \n",
    "    \n",
    "#####  check leaf node\n",
    "    if isTree(myTree['left']):                                          # if the leaf node is a tree, then run plotTree function\n",
    "        plotTree.yOff = plotTree.yOff - 1.0/plotTree.totalD  #*factorY     # update yOff for leaf node\n",
    "        plotTree(myTree['left'], cntrPt, '', factorX, factorY)\n",
    "    else:                                                               # if the leaf node is a leaf node, then plot the node\n",
    "        plotTree.xOff = plotTree.xOff + 1.0/plotTree.totalW             # update xOff for leaf node  !!!!!!!!\n",
    "        plotTree.yOff = plotTree.yOff - 1.0/plotTree.totalD  #*factorY     # update yOff for leaf node\n",
    "        leftNode = round(myTree['left'],2)                              # calculate value for leaf node\n",
    "        plotNode(str(leftNode), (plotTree.xOff, plotTree.yOff), cntrPt, leafNode)      # plot left node\n",
    "        plotMidText((plotTree.xOff, plotTree.yOff), cntrPt, \"<=\")                      # add \"<=\" \n",
    "        \n",
    "    if isTree(myTree['right']):                                         # if the leaf node is a tree, then run plotTree function\n",
    "        plotTree(myTree['right'], cntrPt, '', factorX, factorY)\n",
    "    else:                                                               # if the leaf node is a leaf node, then plot the node\n",
    "        plotTree.xOff = plotTree.xOff + 1.0/plotTree.totalW             # update xOff for leaf node\n",
    "        rightNode = round(myTree['right'],2)                            # calculate value for leaf node\n",
    "        plotNode(str(rightNode), (plotTree.xOff, plotTree.yOff), cntrPt, leafNode)      # plot left node\n",
    "        plotMidText((plotTree.xOff, plotTree.yOff), cntrPt, \">\")                        # add \"<=\" \n",
    "    plotTree.yOff = plotTree.yOff + 1.0/plotTree.totalD #*factorY                         # go back to the last stage   \n",
    "    \n",
    "    \n",
    "def createPlot(inTree, factorX=1, factorY=1):\n",
    "    fig = plt.figure(1, facecolor='white')                                                  # create fig\n",
    "    fig.clf()                                                                               # clear fig\n",
    "    axprops = dict(xticks=[], yticks=[])\n",
    "    createPlot.ax1 = plt.subplot(111, frameon=False, **axprops)                             # delete x、y轴\n",
    "    sumLeafNodes = getNumLeafs(inTree)\n",
    "    treeDepth = getTreeDepth(inTree)\n",
    "    plotTree.totalW = float(getNumLeafs(inTree))/factorX                                    # get total numbe of leaf nodes\n",
    "    plotTree.totalD = float(getTreeDepth(inTree))/factorY                                           # get depth of tree\n",
    "    plotTree.xOff = -0.5/plotTree.totalW; plotTree.yOff = 1.0;                              # define offset for x, y\n",
    "    parent = (plotTree.xOff + (1.0 + float(getNumLeafs(inTree)))/2.0/plotTree.totalW, plotTree.yOff) \n",
    "    print(\"sum of leaf nodes: {} \\ttree depth: {} \\tfactorX: {}\\tfactorY: {}\".format(sumLeafNodes, treeDepth,factorX, factorY))\n",
    "    plotTree(inTree, parent, '', factorX, factorY)                                       # plot tree\n",
    "    plt.show()      \n",
    "    \n",
    "    \n",
    "##########################################################################################\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotTrack(posArr_org, posArr_pred=None, fig_size = (10,10)):\n",
    "    fig = plt.figure(1, figsize=fig_size)\n",
    "#    axprops = dict(xticks=[], yticks=[])\n",
    "#    ax = plt.subplot(111, frameon=False, **axprops)          # delete x- and y-axis\n",
    "    ax = plt.subplot(111)                                     # keep x- and y-axis\n",
    "    posBegEnd = dict(boxstyle=\"round4\", fc=\"0.8\")            # set begiing and ending format\n",
    "\n",
    "    minPosX_org = np.amin(posArr_org[:,0]); maxPosX_org = np.amax(posArr_org[:,0]);  # calculate min and max value of x-axis\n",
    "    minPosY_org = np.amin(posArr_org[:,1]); maxPosY_org = np.amax(posArr_org[:,1]);  # calculate min and max value of y-axis\n",
    "\n",
    "    for i in range(np.shape(posArr_org)[0]-1):     # plot the original route\n",
    "        ax.annotate(\"\",\n",
    "                    xy=posArr_org[i+1], xycoords='data',        # xy: goal\n",
    "                    xytext=posArr_org[i], textcoords='data',    # xytest: parent\n",
    "                    bbox=posBegEnd,\n",
    "                    arrowprops=dict(arrowstyle='-|>',\n",
    "                    connectionstyle=\"arc3\"),\n",
    "        ) \n",
    "    ax.scatter(posArr_org[0][0], posArr_org[0][1], s = 100, c = 'green',alpha = 0.8, marker='s')       # plot the beginning point\n",
    "    ax.scatter(posArr_org[-1][0], posArr_org[-1][1], s = 100, c = 'green',alpha = 1, marker='o')      # plot the ending point\n",
    "    \n",
    "    \n",
    "    \n",
    "    if (np.all(posArr_pred != None)):       # plot the predicted route\n",
    "        for i in range(np.shape(posArr_pred)[0]-1):     \n",
    "            ax.annotate(\"\",\n",
    "                        xy=posArr_pred[i+1], xycoords='data',        # xy: goal\n",
    "                        xytext=posArr_pred[i], textcoords='data',    # xytest: parent\n",
    "                        bbox=posBegEnd,\n",
    "                        arrowprops=dict(arrowstyle='-|>',\n",
    "                        connectionstyle=\"arc3\", color='b'),\n",
    "            ) \n",
    "        ax.scatter(posArr_pred[0][0], posArr_pred[0][1], s = 100, c = 'red',alpha = 0.4, marker='s')\n",
    "        ax.scatter(posArr_pred[-1][0], posArr_pred[-1][1], s = 100, c = 'red',alpha = 1, marker='o')      # plot the ending point\n",
    "        minPosX_pred = np.amin(posArr_pred[:,0]); maxPosX_pred = np.amax(posArr_pred[:,0]);  # calculate min and max value of x-axis\n",
    "        minPosY_pred = np.amin(posArr_pred[:,1]); maxPosY_pred = np.amax(posArr_pred[:,1]);  # calculate min and max value of y-axis\n",
    "        minX = minPosX_org if minPosX_org <= minPosX_pred  else minPosX_pred\n",
    "        maxX = maxPosX_org if maxPosX_org >  maxPosX_pred  else maxPosX_pred\n",
    "        minY = minPosY_org if minPosY_org <= minPosY_pred  else minPosY_pred\n",
    "        maxY = maxPosY_org if maxPosY_org >  maxPosY_pred  else maxPosY_pred\n",
    "        plt.xlim(minX*0.5, maxX*1.5)    # set the limit of x-axis\n",
    "        plt.ylim(minY*0.5, maxY*1.5)    # set the limit of y-axis\n",
    "    \n",
    "    else:\n",
    "        plt.xlim(minPosX_org*0.5, maxPosX_org*1.5)    # set the limit of x-axis\n",
    "        plt.ylim(minPosY_org*0.5, maxPosY_org*1.5)    # set the limit of y-axis\n",
    "        \n",
    "    ax.set_ylim(ax.get_ylim()[::-1])      # invert the y-axis\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\envs\\trajectory\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ndef read_hdf5_auto(Filename):\\n    file = h5py.File(Filename)\\n    main_key = (list(file)[0])                  # \"sample_run\"\\n    group = file.get(main_key)                  # main group\\n  \\n    print(\\'Filename is: \\n\\', Filename)\\n    print(\\'main_key: \\n\\', (list(file)[0]))\\n    print(\\'list(file.get(main_key)): \\n\\', list(file.get(main_key)))\\n    \\n    for sub in list(file.get(main_key)):        # [\\'1\\', \\'2\\',...]\\n        accerlation, position, velocity = read_parameter(group, sub)\\n\\n        print(\\'sub: \\', sub)                     \\n        print(\\'position: \\n\\',position)        \\n        print(\\'\\n\\') \\n        print(\\'velocity: \\n\\',velocity)        \\n        print(\\'\\n\\') \\n        print(\\'accerlation: \\n\\',accerlation)\\n        print(\\'\\n\\')    \\n\\n    return accerlation, position, velocity\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import pandas as pd\n",
    "\n",
    "def read_parameter(group, sub):\n",
    "\n",
    "    sub_group = group.get(sub)   \n",
    "    position = np.array(sub_group.get('Positions'))\n",
    "    velocity = np.array(sub_group.get('Velocity'))\n",
    "    accerlation = np.array(sub_group.get('Acceleration'))\n",
    "    \n",
    "    return accerlation, position, velocity\n",
    "\n",
    "def read_csv(Filename):\n",
    "    data = pd.read_csv(Filename)\n",
    "    print(data)\n",
    "\n",
    "'''\n",
    "def read_hdf5_auto(Filename):\n",
    "    file = h5py.File(Filename)\n",
    "    main_key = (list(file)[0])                  # \"sample_run\"\n",
    "    group = file.get(main_key)                  # main group\n",
    "  \n",
    "    print('Filename is: \\n', Filename)\n",
    "    print('main_key: \\n', (list(file)[0]))\n",
    "    print('list(file.get(main_key)): \\n', list(file.get(main_key)))\n",
    "    \n",
    "    for sub in list(file.get(main_key)):        # ['1', '2',...]\n",
    "        accerlation, position, velocity = read_parameter(group, sub)\n",
    "\n",
    "        print('sub: ', sub)                     \n",
    "        print('position: \\n',position)        \n",
    "        print('\\n') \n",
    "        print('velocity: \\n',velocity)        \n",
    "        print('\\n') \n",
    "        print('accerlation: \\n',accerlation)\n",
    "        print('\\n')    \n",
    "\n",
    "    return accerlation, position, velocity\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read File and prepare the data ready for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'samples/'\n",
    "#Filename = 'mid.hdf5'   #  cub_400.hdf5\n",
    "Filename = 'mid_PR20_PO20_F1_R10_mix.hdf5'   #  cub_400.hdf5\n",
    "#Filename = 'mid_PR0_PO0_F1_R1.hdf5'\n",
    "file = h5py.File(path + Filename)\n",
    "main_key = (list(file)[0])                  # \"sample_run\"\n",
    "group = file.get(main_key)                  # main group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mid_PR20_PO20_F1_R10'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mid_PR20_PO20_F1_R10'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 group \"/mid_PR20_PO20_F1_R10\" (70 members)>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mid_PR20_PO20_F1_R10']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " '10',\n",
       " '11',\n",
       " '12',\n",
       " '13',\n",
       " '14',\n",
       " '15',\n",
       " '16',\n",
       " '17',\n",
       " '18',\n",
       " '19',\n",
       " '2',\n",
       " '20',\n",
       " '21',\n",
       " '22',\n",
       " '23',\n",
       " '24',\n",
       " '25',\n",
       " '26',\n",
       " '27',\n",
       " '28',\n",
       " '29',\n",
       " '3',\n",
       " '30',\n",
       " '31',\n",
       " '32',\n",
       " '33',\n",
       " '34',\n",
       " '35',\n",
       " '36',\n",
       " '37',\n",
       " '38',\n",
       " '39',\n",
       " '4',\n",
       " '40',\n",
       " '41',\n",
       " '42',\n",
       " '43',\n",
       " '44',\n",
       " '45',\n",
       " '46',\n",
       " '47',\n",
       " '48',\n",
       " '49',\n",
       " '5',\n",
       " '50',\n",
       " '51',\n",
       " '52',\n",
       " '53',\n",
       " '54',\n",
       " '55',\n",
       " '56',\n",
       " '57',\n",
       " '58',\n",
       " '59',\n",
       " '6',\n",
       " '60',\n",
       " '61',\n",
       " '62',\n",
       " '63',\n",
       " '64',\n",
       " '65',\n",
       " '66',\n",
       " '67',\n",
       " '68',\n",
       " '69',\n",
       " '7',\n",
       " '70',\n",
       " '8',\n",
       " '9']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(list(file.get(main_key)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 group \"/mid_PR20_PO20_F1_R10/1\" (3 members)>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_group = group.get('1')  \n",
    "sub_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sub_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sub_group.get('Positions'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-96-e25a4c29dced>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msub_group\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'velocity'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "len(sub_group.get('velocity'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-95-9dbc5732c95a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msub_group\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'accerlation'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "len(sub_group.get('accerlation'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 198.,  201.],\n",
       "       [ 284.,  202.],\n",
       "       [ 303.,  318.],\n",
       "       [ 431.,  300.],\n",
       "       [ 382.,  329.],\n",
       "       [ 460.,  369.],\n",
       "       [ 439.,  472.],\n",
       "       [ 536.,  542.],\n",
       "       [ 568.,  602.],\n",
       "       [ 698.,  625.],\n",
       "       [ 716.,  752.],\n",
       "       [ 713.,  724.],\n",
       "       [ 804.,  841.],\n",
       "       [ 860.,  821.],\n",
       "       [ 895.,  946.],\n",
       "       [ 942.,  909.],\n",
       "       [1001.,  988.]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position = np.array(sub_group.get('Positions'))\n",
    "position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "velocity = np.array(sub_group.get('Velocity'))\n",
    "len(velocity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accerlation = np.array(sub_group.get('Acceleration'))\n",
    "len(accerlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items in the base directory: \n",
      " [('mid_PR20_PO20_F1_R10', <HDF5 group \"/mid_PR20_PO20_F1_R10\" (70 members)>)]\n",
      "\n",
      "\n",
      "Items in group: \n",
      " [('1', <HDF5 group \"/mid_PR20_PO20_F1_R10/1\" (3 members)>), ('10', <HDF5 group \"/mid_PR20_PO20_F1_R10/10\" (3 members)>), ('11', <HDF5 group \"/mid_PR20_PO20_F1_R10/11\" (3 members)>), ('12', <HDF5 group \"/mid_PR20_PO20_F1_R10/12\" (3 members)>), ('13', <HDF5 group \"/mid_PR20_PO20_F1_R10/13\" (3 members)>), ('14', <HDF5 group \"/mid_PR20_PO20_F1_R10/14\" (3 members)>), ('15', <HDF5 group \"/mid_PR20_PO20_F1_R10/15\" (3 members)>), ('16', <HDF5 group \"/mid_PR20_PO20_F1_R10/16\" (3 members)>), ('17', <HDF5 group \"/mid_PR20_PO20_F1_R10/17\" (3 members)>), ('18', <HDF5 group \"/mid_PR20_PO20_F1_R10/18\" (3 members)>), ('19', <HDF5 group \"/mid_PR20_PO20_F1_R10/19\" (3 members)>), ('2', <HDF5 group \"/mid_PR20_PO20_F1_R10/2\" (3 members)>), ('20', <HDF5 group \"/mid_PR20_PO20_F1_R10/20\" (3 members)>), ('21', <HDF5 group \"/mid_PR20_PO20_F1_R10/21\" (3 members)>), ('22', <HDF5 group \"/mid_PR20_PO20_F1_R10/22\" (3 members)>), ('23', <HDF5 group \"/mid_PR20_PO20_F1_R10/23\" (3 members)>), ('24', <HDF5 group \"/mid_PR20_PO20_F1_R10/24\" (3 members)>), ('25', <HDF5 group \"/mid_PR20_PO20_F1_R10/25\" (3 members)>), ('26', <HDF5 group \"/mid_PR20_PO20_F1_R10/26\" (3 members)>), ('27', <HDF5 group \"/mid_PR20_PO20_F1_R10/27\" (3 members)>), ('28', <HDF5 group \"/mid_PR20_PO20_F1_R10/28\" (3 members)>), ('29', <HDF5 group \"/mid_PR20_PO20_F1_R10/29\" (3 members)>), ('3', <HDF5 group \"/mid_PR20_PO20_F1_R10/3\" (3 members)>), ('30', <HDF5 group \"/mid_PR20_PO20_F1_R10/30\" (3 members)>), ('31', <HDF5 group \"/mid_PR20_PO20_F1_R10/31\" (3 members)>), ('32', <HDF5 group \"/mid_PR20_PO20_F1_R10/32\" (3 members)>), ('33', <HDF5 group \"/mid_PR20_PO20_F1_R10/33\" (3 members)>), ('34', <HDF5 group \"/mid_PR20_PO20_F1_R10/34\" (3 members)>), ('35', <HDF5 group \"/mid_PR20_PO20_F1_R10/35\" (3 members)>), ('36', <HDF5 group \"/mid_PR20_PO20_F1_R10/36\" (3 members)>), ('37', <HDF5 group \"/mid_PR20_PO20_F1_R10/37\" (3 members)>), ('38', <HDF5 group \"/mid_PR20_PO20_F1_R10/38\" (3 members)>), ('39', <HDF5 group \"/mid_PR20_PO20_F1_R10/39\" (3 members)>), ('4', <HDF5 group \"/mid_PR20_PO20_F1_R10/4\" (3 members)>), ('40', <HDF5 group \"/mid_PR20_PO20_F1_R10/40\" (3 members)>), ('41', <HDF5 group \"/mid_PR20_PO20_F1_R10/41\" (3 members)>), ('42', <HDF5 group \"/mid_PR20_PO20_F1_R10/42\" (3 members)>), ('43', <HDF5 group \"/mid_PR20_PO20_F1_R10/43\" (3 members)>), ('44', <HDF5 group \"/mid_PR20_PO20_F1_R10/44\" (3 members)>), ('45', <HDF5 group \"/mid_PR20_PO20_F1_R10/45\" (3 members)>), ('46', <HDF5 group \"/mid_PR20_PO20_F1_R10/46\" (3 members)>), ('47', <HDF5 group \"/mid_PR20_PO20_F1_R10/47\" (3 members)>), ('48', <HDF5 group \"/mid_PR20_PO20_F1_R10/48\" (3 members)>), ('49', <HDF5 group \"/mid_PR20_PO20_F1_R10/49\" (3 members)>), ('5', <HDF5 group \"/mid_PR20_PO20_F1_R10/5\" (3 members)>), ('50', <HDF5 group \"/mid_PR20_PO20_F1_R10/50\" (3 members)>), ('51', <HDF5 group \"/mid_PR20_PO20_F1_R10/51\" (3 members)>), ('52', <HDF5 group \"/mid_PR20_PO20_F1_R10/52\" (3 members)>), ('53', <HDF5 group \"/mid_PR20_PO20_F1_R10/53\" (3 members)>), ('54', <HDF5 group \"/mid_PR20_PO20_F1_R10/54\" (3 members)>), ('55', <HDF5 group \"/mid_PR20_PO20_F1_R10/55\" (3 members)>), ('56', <HDF5 group \"/mid_PR20_PO20_F1_R10/56\" (3 members)>), ('57', <HDF5 group \"/mid_PR20_PO20_F1_R10/57\" (3 members)>), ('58', <HDF5 group \"/mid_PR20_PO20_F1_R10/58\" (3 members)>), ('59', <HDF5 group \"/mid_PR20_PO20_F1_R10/59\" (3 members)>), ('6', <HDF5 group \"/mid_PR20_PO20_F1_R10/6\" (3 members)>), ('60', <HDF5 group \"/mid_PR20_PO20_F1_R10/60\" (3 members)>), ('61', <HDF5 group \"/mid_PR20_PO20_F1_R10/61\" (3 members)>), ('62', <HDF5 group \"/mid_PR20_PO20_F1_R10/62\" (3 members)>), ('63', <HDF5 group \"/mid_PR20_PO20_F1_R10/63\" (3 members)>), ('64', <HDF5 group \"/mid_PR20_PO20_F1_R10/64\" (3 members)>), ('65', <HDF5 group \"/mid_PR20_PO20_F1_R10/65\" (3 members)>), ('66', <HDF5 group \"/mid_PR20_PO20_F1_R10/66\" (3 members)>), ('67', <HDF5 group \"/mid_PR20_PO20_F1_R10/67\" (3 members)>), ('68', <HDF5 group \"/mid_PR20_PO20_F1_R10/68\" (3 members)>), ('69', <HDF5 group \"/mid_PR20_PO20_F1_R10/69\" (3 members)>), ('7', <HDF5 group \"/mid_PR20_PO20_F1_R10/7\" (3 members)>), ('70', <HDF5 group \"/mid_PR20_PO20_F1_R10/70\" (3 members)>), ('8', <HDF5 group \"/mid_PR20_PO20_F1_R10/8\" (3 members)>), ('9', <HDF5 group \"/mid_PR20_PO20_F1_R10/9\" (3 members)>)]\n",
      "\n",
      "\n",
      "Items in data_group: \n",
      " [('Acceleration', <HDF5 dataset \"Acceleration\": shape (15, 2), type \"<f8\">), ('Positions', <HDF5 dataset \"Positions\": shape (17, 2), type \"<f8\">), ('Velocity', <HDF5 dataset \"Velocity\": shape (16, 2), type \"<f8\">)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(path + Filename, 'r') as hdf:\n",
    "    base_items = list(hdf.items())\n",
    "    print('Items in the base directory: \\n', base_items)\n",
    "    print('\\n')\n",
    "    group = hdf.get('mid_PR20_PO20_F1_R10')\n",
    "    group_items = list(group.items())\n",
    "    print('Items in group: \\n',group_items)\n",
    "    print('\\n')\n",
    "    data_group = group.get('1')\n",
    "    data_items = list(data_group.items())\n",
    "    print('Items in data_group: \\n',data_items)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = []\n",
    "a.__sizeof__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.append('a')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
